{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Planning**   - ","metadata":{}},{"cell_type":"markdown","source":"****Please run Offline learning code first-> I am providing code below ,and you can also refer Bhavya Pranav Tandra's code ****","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n\n#from rolf.utils import LinearDecay\n#horizon_decay = LinearDecay()\nbatch = pretrain_buffer.sample(batch_size)\nob, ac = batch[\"ob\"], batch[\"ac\"]\n#meta_agent = SkiMoMetaAgent(cfg, ob_space, meta_ac_space)\n#skill_agent.set_step(step)\n\n\n\n# This is for estimate_value\nclass Critic(nn.Module):\n    def __init__(self, input_dim, hidden_dims, ensemble, activation):\n        super().__init__()\n        self._ensemble = ensemble\n        h = hidden_dims\n        assert len(h) > 0\n        self.fcs = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.Linear(input_dim, h[0]),\n                    nn.LayerNorm(h[0]),\n                    nn.Tanh(),\n                    MLP(h[0], 1, h[1:], activation, small_weight=True),\n                )\n                for _ in range(ensemble)\n            ]\n        )\n\n    def forward(self, state, ac=None):\n        if ac is not None:\n            state = torch.cat([state, ac], dim=-1)\n        q = [fc(state).squeeze(-1) for fc in self.fcs]\n        return q[0] if self._ensemble == 1 else q\n    \ncritic = Critic(50+10,[256]*4 ,1, \"elu\")\n\n@torch.no_grad()  \ndef estimate_value(self, state, ac, horizon):\n    \"\"\"Imagine a trajectory for `horizon` steps, and estimate the value.\"\"\"\n    value, discount = 0, 1\n    for t in range(horizon):\n        #state, reward = self.model.imagine_step(state, ac[t])\n        reward = crtic(state, ac[t])\n        state = dynamics(state, ac[t])\n        value += discount * reward\n        discount *= 0.99  #rl_discount: 0.99\n    value += discount * torch.min(*critic(state, skill_policy(state)))\n    return value\n\n\n\ndef plan(ob, prev_mean=None, is_train=True):  # this method is responsible for \n        \"\"\"Plan given an observation `ob`.\"\"\"\n        #cfg = self._cfg\n        horizon = int(horizon_decay(step=0))\n        #horizon_decay = LinearDecay(1, cfg.n_skill, cfg.horizon_step)\n\n        #state = self.model.encoder(ob)\n        state =state_encoder(ob)  # you need run this \n        \n        # Sample policy trajectories.\n        z = state.repeat(num_policy_traj, 1)\n        policy_ac = []\n        for t in range(horizon):\n            #policy_ac.append(self.actor.act(z))\n            \n            policy_ac.append(skill_policy(z)) # it will call forward method\n            z = dynamics(z, policy_ac[t])\n        policy_ac = torch.stack(policy_ac, dim=0)\n\n        # CEM optimization.\n        z = state.repeat(num_policy_traj + num_sample_traj, 1)\n        mean = torch.zeros(horizon, 10, device=device) # ac_dim =skill_dim: 10\n        std = 2.0 * torch.ones(horizon, 10, device=device)\n        if prev_mean is not None and horizon > 1 and prev_mean.shape[0] == horizon:\n            mean[:-1] = prev_mean[1:]\n\n        for _ in range(6):# cem_iter: 6 , we can have differnt graphs ,if you change cem_iter \n            # cem_iter \n            sample_ac = mean.unsqueeze(1) + std.unsqueeze(1) * torch.randn(\n                horizon, 512, 10, device=device # ac_dim =skill_dim: 10 ,num_sample_traj: 512\n            )\n            sample_ac = torch.clamp(sample_ac, -0.999, 0.999)\n\n            ac = torch.cat([sample_ac, policy_ac], dim=1)\n\n            imagine_return = estimate_value(z, ac, horizon).squeeze(-1)\n            _, idxs = imagine_return.sort(dim=0)\n            idxs = idxs[-64] # num_elites: 64\n            elite_value = imagine_return[idxs]\n            elite_action = ac[:, idxs]\n\n            # Weighted aggregation of elite plans.\n            score = torch.exp(0.5 * (elite_value - elite_value.max())) # cem_temperature: 0.5\n            score = (score / score.sum()).view(1, -1, 1)\n            new_mean = (score * elite_action).sum(dim=1)\n            new_std = torch.sqrt(\n                torch.sum(score * (elite_action - new_mean.unsqueeze(1)) ** 2, dim=1)\n            )\n\n            mean =  0.1* mean + (1 - 0.1) * new_mean # cem_momentum: 0.1\n            \n            std = torch.clamp(new_std,std_decay(step=0), 2) # num_elites: 64\n\n        # Sample action for MPC.\n        score = score.squeeze().cpu().numpy()\n        ac = elite_action[0, np.random.choice(np.arange(64), p=score)] #num_elites: 64\n        if is_train:\n            ac += std[0] * torch.randn_like(std[0])\n        return torch.clamp(ac, -0.999, 0.999), mean\n    \n    \n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here I need learned perameter from Downstream RL , for that i need results from my parter but he not able to provide in given so i did it ummy implementation,\n\n# I think I can do Downstream RL then run planning ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Dummy implementations \ndef state_encoder(ob):\n    return torch.randn(1, 50)\n\ndef skill_policy(z):\n    return torch.randn(1, 10)\n\ndef dynamics(z, policy_ac):\n    return z + policy_ac\n\ndef estimate_value(z, ac, horizon):\n    return torch.randn(len(ac))\n\ndef horizon_decay(step):\n    return 10  # Replace with actual implementation\n\ndef std_decay(step):\n    return 1  # Replace with actual implementation\n\n# The planning function with CEM iterations and distribution data collection\ndef plan(ob, prev_mean=None, is_train=True):\n    horizon = horizon_decay(step=0)\n    state = state_encoder(ob)\n    num_policy_traj = 100  # Number of policy trajectories to sample\n    num_sample_traj = 512  # Number of sample trajectories for CEM\n    device = device  \n    \n    z = state.repeat(num_policy_traj, 1)\n    policy_ac = []\n    for t in range(horizon):\n        policy_ac.append(skill_policy(z))\n        z = dynamics(z, policy_ac[t])\n    policy_ac = torch.stack(policy_ac, dim=0)\n    \n    z = state.repeat(num_policy_traj + num_sample_traj, 1)\n    mean = torch.zeros(horizon, 10, device=device)\n    std = 2.0 * torch.ones(horizon, 10, device=device)\n    \n    mus, sigmas, all_samples = [], [], []\n    for _ in range(8):  # cem_iter: 8\n        sample_ac = mean.unsqueeze(1) + std.unsqueeze(1) * torch.randn(\n            horizon, num_sample_traj, 10, device=device\n        )\n        sample_ac = torch.clamp(sample_ac, -0.999, 0.999)\n        \n        ac = torch.cat([sample_ac, policy_ac], dim=1)\n        imagine_return = estimate_value(z, ac, horizon).squeeze(-1)\n        _, idxs = imagine_return.topk(64)  # num_elites: 64\n        elite_value = imagine_return[idxs]\n        elite_action = ac[:, idxs]\n        \n        score = F.softmax(0.5 * (elite_value - elite_value.max()), dim=0)\n        new_mean = (score.unsqueeze(-1) * elite_action).sum(dim=1)\n        new_std = torch.sqrt((score.unsqueeze(-1) * (elite_action - new_mean.unsqueeze(1)) ** 2).sum(dim=1))\n        \n        mean = 0.1 * mean + (1 - 0.1) * new_mean\n        std = torch.clamp(new_std, std_decay(step=0), 2)\n        \n        mus.append(mean.squeeze().detach().cpu().numpy())\n        sigmas.append(std.squeeze().detach().cpu().numpy())\n        all_samples.append(sample_ac.squeeze().detach().cpu().numpy())\n\n    return mus, sigmas, all_samples\n\n# Run the plan function to get the distributions\nob = torch.randn(1, 50)  # Dummy observation\nmus, sigmas, all_samples = plan(ob)\n\n# Plotting the Gaussian distributions\nfor i in range(8):\n    plt.figure(figsize=(10, 5))\n    sample_points = all_samples[i]\n    mu, sigma = mus[i], sigmas[i]\n    count, bins, ignored = plt.hist(sample_points, 30, density=True, alpha=0.5, color='g')\n    plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ),linewidth=2, color='r')\n    plt.title(f'CEM iter={i}')\n    plt.xlabel('Skill Sequences')\n    plt.ylabel('Probability Density')\n    plt.grid(True)\n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **Offline learning**","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I need use whole cell for ob -> \nfrom collections import defaultdict, deque\nfrom functools import partial\n\nimport numpy as np\nimport tensorflow as tf\nimport torch.utils.data\n\n\n\n\n\n\ndef _convert(value, precision):\n    if isinstance(value, dict):\n        return {k: _convert(v, precision) for k, v in value.items()}\n\n    value = np.array(value)\n    if np.issubdtype(value.dtype, np.floating):\n        dtype = {16: np.float16, 32: np.float32}[precision]\n    elif np.issubdtype(value.dtype, np.signedinteger):\n        dtype = {16: np.int16, 32: np.int32}[precision]\n    else:\n        dtype = value.dtype\n    return value.astype(dtype)\n\n\nclass ReplayBuffer(object):\n    \"\"\"Replay buffer to store trainsitions in list (deque).\"\"\"\n\n    def __init__(self, keys, buffer_size, sample_func):\n        self._capacity = buffer_size\n        self._sample_func = sample_func\n\n        # create the buffer to store info\n        self._keys = keys\n        self.clear()\n\n    @property\n    def size(self):\n        return self._capacity\n\n    @property\n    def last_saved_idx(self):\n        return self._last_saved_idx\n\n    def clear(self):\n        self._idx = 0\n        self._current_size = 0\n        self._last_saved_idx = -1\n        self._buffer = defaultdict(partial(deque, maxlen=self._capacity))\n\n    def store_episode(self, rollout):\n        \"\"\"`rollout` can be any length of transitions.\"\"\"\n        for k in self._keys:\n            self._buffer[k].append(rollout[k])\n\n        self._idx += 1\n        if self._current_size < self._capacity:\n            self._current_size += 1\n\n    def sample(self, batch_size):\n        transitions = self._sample_func(self._buffer, batch_size)\n        return transitions\n\n    def state_dict(self):\n        \"\"\"Returns new transitions in replay buffer.\"\"\"\n        assert self._idx - self._last_saved_idx - 1 <= self._capacity\n        state_dict = {}\n        s = (self._last_saved_idx + 1) % self._capacity\n        e = (self._idx - 1) % self._capacity\n        for k in self._keys:\n            state_dict[k] = list(self._buffer[k])\n            if s < e:\n                state_dict[k] = state_dict[k][s : e + 1]\n            else:\n                state_dict[k] = state_dict[k][s:] + state_dict[k][: e + 1]\n            assert len(state_dict[k]) == self._idx - self._last_saved_idx - 1\n        self._last_saved_idx = self._idx - 1\n        \n        return state_dict\n\n    def append_state_dict(self, state_dict):\n        \"\"\"Adds transitions to replay buffer.\"\"\"\n        for k in self._keys:\n            self._buffer[k].extend(state_dict[k])\n\n        n = len(state_dict[\"ac\"])\n        self._last_saved_idx += n\n        self._idx += n\n        \n\n\nclass ReplayBufferEpisode(object):\n    \"\"\"Replay buffer storing each episode in file.\"\"\"\n\n    def __init__(self, keys, buffer_size, sample_func, precision=32):\n        super().__init__()\n        self._capacity = buffer_size\n        self._sample_func = sample_func\n        self._precision = precision\n\n        # create the buffer to store info\n        self._keys = keys\n        self.clear()\n\n    @property\n    def size(self):\n        return self._capacity\n\n    @property\n    def buffer(self):\n        return self._buffer\n\n    def clear(self):\n        self._idx = 0\n        self._new_episode = True\n        self._last_saved_idx = -1\n        self._buffer = deque(maxlen=self._capacity)\n        self._rollout = {k: [] for k in self._keys}\n\n    def store_episode(self, rollout, include_last_ob=True, one_more_ob=False):\n        \"\"\"Stores `rollout` into `self._buffer`.\n\n        Args:\n            rollout: A dictionary of lists of transitions.\n            include_last_ob: Let each episode include the last observations, such that len(ob) = len(ac) + 1.\n            one_more_ob: A collected rollout will contain one more observations than actions.\n        \"\"\"\n        assert \"done\" in rollout\n        rollout_len = len(rollout[\"done\"])\n        for i in range(rollout_len):\n            if one_more_ob and include_last_ob and len(self._rollout[\"ob\"]) == 0:\n                # Add the initial observation.\n                self._rollout[\"ob\"].append(rollout[\"ob\"].pop(i))\n\n            for k in self._keys:\n                self._rollout[k].append(rollout[k][i])\n\n            if rollout[\"done\"][i]:\n                # Add the last observation.\n                if include_last_ob:\n                    if \"ob_next\" in rollout:\n                        self._rollout[\"ob\"].append(rollout[\"ob_next\"][i])\n                    elif not one_more_ob:\n                        for k in self._keys:\n                            if k != \"ob\":\n                                self._rollout[k].pop()\n                        self._rollout[\"done\"][-1] = 1\n\n                episode = {}\n                for k in self._keys:\n                    episode[k] = {}\n                    if isinstance(self._rollout[k][0], dict):\n                        for sub_key in self._rollout[k][0]:\n                            v = np.array([t[sub_key] for t in self._rollout[k]])\n                            v = _convert(v, self._precision)\n                            episode[k][sub_key] = torch.as_tensor(v)\n                    else:\n                        v = np.array(self._rollout[k])\n                        episode[k] = torch.as_tensor(_convert(v, self._precision))\n\n                self._buffer.append(episode)\n                self._idx += 1\n                self._rollout = {k: [] for k in self._keys}\n\n    def sample(self, batch_size):\n        batch = self._sample_func(self._buffer, batch_size)\n        return batch\n\n    def state_dict(self):\n        \"\"\"Returns new transitions in replay buffer.\"\"\"\n        assert self._idx - self._last_saved_idx - 1 <= self._capacity\n        state_dict = {}\n        s = (self._last_saved_idx + 1) % self._capacity\n        e = (self._idx - 1) % self._capacity\n        l = list(self._buffer)\n        if s <= e:\n            state_dict = {\"episodes\": l[s : e + 1]}\n        else:\n            state_dict = {\"episodes\": l[s:] + l[: e + 1]}\n        self._last_saved_idx = self._idx - 1\n        \n        return state_dict\n\n    def append_state_dict(self, state_dict):\n        \"\"\"Adds transitions to replay buffer.\"\"\"\n        if state_dict is None:\n            return\n        self._buffer.extend(state_dict[\"episodes\"])\n\n        n = len(state_dict[\"episodes\"])\n        self._last_saved_idx += n\n        self._idx += n\n        \n\n\nclass RandomSampler(object):\n    def __init__(self):\n        pass\n\n    def sample_func(self, dataset, batch_size):\n        key_len = \"ac\" if \"ac\" in dataset else \"ob\"\n        dataset_size = len(dataset[key_len])\n\n        episode_idxs = np.random.randint(0, dataset_size, batch_size)\n        t_samples = [\n            np.random.randint(len(dataset[key_len][episode_idx]))\n            for episode_idx in episode_idxs\n        ]\n\n        transitions = {}\n        for key in dataset.keys():\n            transitions[key] = [\n                dataset[key][episode_idx][t]\n                for episode_idx, t in zip(episode_idxs, t_samples)\n            ]\n\n        new_transitions = {}\n        for k, v in transitions.items():\n            if isinstance(v[0], dict):\n                sub_keys = v[0].keys()\n                new_transitions[k] = {\n                    sub_key: np.stack([v_[sub_key] for v_ in v]) for sub_key in sub_keys\n                }\n            else:\n                new_transitions[k] = np.stack(v)\n\n        return new_transitions\n\n\nclass SeqSampler(object):\n    def __init__(self, seq_len, sample_last_more=False):\n        self._seq_len = seq_len\n        self._sample_last_more = sample_last_more\n\n    def sample_func(self, dataset, batch_size):\n        data = []\n        dataset_len = len(dataset)\n        for _ in range(batch_size):\n            while True:\n                episode_idx = np.random.randint(dataset_len)\n                episode = dataset[episode_idx].copy()\n                episode_len = len(episode[\"done\"])\n                if episode_len >= self._seq_len:\n                    break\n\n            t = np.random.randint(episode_len - self._seq_len + 1)\n            traj = [v[t : t + self._seq_len] for v in tf.nest.flatten(episode)]\n            data.append(traj)\n\n        batch = [np.stack([v[i] for v in data]) for i in range(len(traj))]\n        return tf.nest.pack_sequence_as(episode, batch)\n\n    def sample_func_tensor(self, dataset, batch_size):\n        \"\"\"Sample batch in tensor.\"\"\"\n        dataset_len = len(dataset)\n        episode = dataset[0].copy()\n        new_tensor = lambda v, l: torch.empty(\n            (batch_size, l, *v.shape[1:]), dtype=v.dtype\n        )\n        batch = [new_tensor(v, self._seq_len) for v in tf.nest.flatten(episode)]\n\n        for i in range(batch_size):\n            while True:\n                episode_idx = np.random.randint(dataset_len)\n                episode = dataset[episode_idx].copy()\n                episode_len = len(episode[\"done\"])\n                if episode_len >= self._seq_len:\n                    break\n\n            t = np.random.randint(episode_len - self._seq_len + 1)\n            for j, v in enumerate(tf.nest.flatten(episode)):\n                batch[j][i] = v[t : t + self._seq_len]\n\n        # TODO: change cuda() to to(self._device)\n        batch = [b.to(device) for b in batch]\n        return tf.nest.pack_sequence_as(episode, batch)\n\n    def sample_func_one_more_ob(self, dataset, batch_size):\n        \"\"\"Sample one more `ob` than other items.\"\"\"\n        # 0.18 sec (numpy array) -> 0.08 sec (empty cuda tensor) -> 0.067 sec (cuda at once)\n        dataset_len = len(dataset)\n        episode = dataset[0].copy()\n        ob = episode.pop(\"ob\")\n        new_tensor = lambda v, l: torch.empty(\n            (batch_size, l, *v.shape[1:]), dtype=v.dtype\n        )\n        batch_ob = [new_tensor(v, self._seq_len + 1) for v in tf.nest.flatten(ob)]\n        batch_ep = [new_tensor(v, self._seq_len) for v in tf.nest.flatten(episode)]\n\n        for i in range(batch_size):\n            while True:\n                episode_idx = np.random.randint(dataset_len)\n                episode = dataset[episode_idx].copy()\n                episode_len = len(episode[\"done\"])\n                if episode_len >= self._seq_len:\n                    break\n\n            if self._sample_last_more:\n                t = np.random.randint(episode_len)\n                t = np.clip(t, 0, episode_len - self._seq_len)\n            else:\n                t = np.random.randint(episode_len - self._seq_len + 1)\n            ob = episode.pop(\"ob\")\n            for j, v in enumerate(tf.nest.flatten(ob)):\n                batch_ob[j][i] = v[t : t + self._seq_len + 1]\n            for j, v in enumerate(tf.nest.flatten(episode)):\n                batch_ep[j][i] = v[t : t + self._seq_len]\n\n        # TODO: change cuda() to to(self._device)\n        batch_ob = [b.to(device) for b in batch_ob]\n        batch_ep = [b.to(device) for b in batch_ep]\n\n        batch_ob = tf.nest.pack_sequence_as(ob, batch_ob)\n        batch_ep = tf.nest.pack_sequence_as(episode, batch_ep)\n        batch_ep.update(dict(ob=batch_ob))\n\n        return batch_ep\n\n    def sample_func_one_more_ob_d4rl(self, dataset, batch_size):\n        \"\"\"Sample one more `ob` than other items.\n        Sampling one more ob when data has the same number of observations and actions.\n        Thus, the last action of each episode will not be used.\n        \"\"\"\n\n        # 0.18 sec (numpy array) -> 0.08 sec (empty cuda tensor) -> 0.067 sec (cuda at once)\n        dataset_len = len(dataset)\n        episode = dataset[0].copy()\n        ob = episode.pop(\"ob\")\n        new_tensor = lambda v, l: torch.empty(\n            (batch_size, l, *v.shape[1:]), dtype=v.dtype\n        )\n        batch_ob = [new_tensor(v, self._seq_len + 1) for v in tf.nest.flatten(ob)]\n        batch_ep = [new_tensor(v, self._seq_len) for v in tf.nest.flatten(episode)]\n\n        for i in range(batch_size):\n            while True:\n                episode_idx = np.random.randint(dataset_len)\n                episode = dataset[episode_idx].copy()\n                episode_len = len(episode[\"done\"])\n                if episode_len > self._seq_len:\n                    break\n\n            t = np.random.randint(episode_len - self._seq_len)\n            ob = episode.pop(\"ob\")\n            for j, v in enumerate(tf.nest.flatten(ob)):\n                batch_ob[j][i] = v[t : t + self._seq_len + 1]\n            for j, v in enumerate(tf.nest.flatten(episode)):\n                batch_ep[j][i] = v[t : t + self._seq_len]\n\n        # TODO: change cuda() to to(self._device)\n        batch_ob = [b.to(device) for b in batch_ob]\n        batch_ep = [b.to(device) for b in batch_ep]\n\n        batch_ob = tf.nest.pack_sequence_as(ob, batch_ob)\n        batch_ep = tf.nest.pack_sequence_as(episode, batch_ep)\n        batch_ep.update(dict(ob=batch_ob))\n\n        return batch_ep\n\n\nclass HERSampler(object):\n    def __init__(self, replay_strategy, replace_future, reward_func=None):\n        self.replay_strategy = replay_strategy\n        if self.replay_strategy == \"future\":\n            self.future_p = replace_future\n        else:\n            self.future_p = 0\n        self.reward_func = reward_func\n\n    def sample_her_transitions(self, dataset, batch_size):\n        key_len = \"ac\" if \"ac\" in dataset else \"ob\"\n        dataset_size = len(dataset[key_len])\n\n        # select which rollouts and which timesteps to be used\n        episode_idxs = np.random.randint(0, dataset_size, batch_size)\n        t_samples = [\n            np.random.randint(len(dataset[\"ac\"][episode_idx]))\n            for episode_idx in episode_idxs\n        ]\n\n        transitions = {}\n        for key in dataset.keys():\n            transitions[key] = [\n                dataset[key][episode_idx][t]\n                for episode_idx, t in zip(episode_idxs, t_samples)\n            ]\n\n        transitions[\"ob_next\"] = [\n            dataset[\"ob\"][episode_idx][t + 1]\n            for episode_idx, t in zip(episode_idxs, t_samples)\n        ]\n        transitions[\"r\"] = np.zeros((batch_size,))\n\n        # hindsight experience replay\n        for i, (episode_idx, t) in enumerate(zip(episode_idxs, t_samples)):\n            replace_goal = np.random.uniform() < self.future_p\n            if replace_goal:\n                future_t = np.random.randint(t + 1, len(dataset[\"ac\"][episode_idx]) + 1)\n                future_ag = dataset[\"ag\"][episode_idx][future_t]\n                if self.reward_func(dataset[\"ag\"][episode_idx][t], future_ag, None) < 0:\n                    transitions[\"g\"][i] = future_ag\n            transitions[\"r\"][i] = self.reward_func(\n                dataset[\"ag\"][episode_idx][t + 1], transitions[\"g\"][i], None\n            )\n\n        new_transitions = {}\n        for k, v in transitions.items():\n            if isinstance(v[0], dict):\n                sub_keys = v[0].keys()\n                new_transitions[k] = {\n                    sub_key: np.stack([v_[sub_key] for v_ in v]) for sub_key in sub_keys\n                }\n            else:\n                new_transitions[k] = np.stack(v)\n\n        return new_transitions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Specify the path to your pickled file\nfile_path = '/kaggle/input/pointmaze/maze_states_skild.pkl'\n\n# Load the data from the pickled file\nwith open(file_path, 'rb') as file:\n    data = pickle.load(file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_size = len(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ac_dim = 2\n#ac_dim = skill_dim\nskill_horizon = 10\nstate_dim = 50\nskill_input_dim = skill_horizon * ac_dim + state_dim\nbatch_size = 256\nskill_dim = 10\nskill_horizon = 10\nn_skill = 10\nhorizon = n_skill * skill_horizon\nsampler = SeqSampler(horizon+1)\npretrain_buffer = ReplayBufferEpisode([\"ob\", \"ac\", \"done\"], None, sampler.sample_func_tensor)\npretrain_val_buffer = ReplayBufferEpisode([\"ob\", \"ac\", \"done\"], None, sampler.sample_func_tensor)\nfor i, d in enumerate(data):\n    if len(d[\"obs\"]) < len(d[\"dones\"]):\n        continue  # Skip incomplete trajectories.\n\n    new_d = dict(ob=d[\"obs\"], ac=d[\"actions\"], done=d[\"dones\"])\n    new_d[\"done\"][-1] = 1.0  # Force last step to be done.\n    \n    if i < data_size * 0.99: #using 0.99 percent of data for training and rest for validaation\n        if i == 0:\n            temp_d = new_d\n        #print(f'Storing Episode: {new_d}')\n        pretrain_buffer.store_episode(new_d, False)\n    else:\n        pretrain_val_buffer.store_episode(new_d, False)\n\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nclass RandomShiftsAug(nn.Module):\n    \"\"\"Random shift image augmentation.\n    Adapted from https://github.com/facebookresearch/drqv2 and https://github.com/nicklashansen/tdmpc\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # self.pad = int(cfg.img_size/21) if cfg.modality == 'pixels' else None\n        self._pad = 4\n\n    def forward(self, x):\n        shape = x.shape\n        x = x.reshape(-1, *shape[-3:])\n        x = x.permute(0, 3, 1, 2)\n        n, c, h, w = x.size()\n        assert h == w\n        padding = tuple([self._pad] * 4)\n        x = F.pad(x, padding, \"replicate\")\n        eps = 1.0 / (h + 2 * self._pad)\n        arange = torch.linspace(\n            -1.0 + eps, 1.0 - eps, h + 2 * self._pad, device=x.device, dtype=x.dtype\n        )[:h]\n        arange = arange.unsqueeze(0).repeat(h, 1).unsqueeze(2)\n        base_grid = torch.cat([arange, arange.transpose(1, 0)], dim=2)\n        base_grid = base_grid.unsqueeze(0).repeat(n, 1, 1, 1)\n        shift = torch.randint(\n            0, 2 * self._pad + 1, size=(n, 1, 1, 2), device=x.device, dtype=x.dtype\n        )\n        shift *= 2.0 / (h + 2 * self._pad)\n        grid = base_grid + shift\n        x = F.grid_sample(x, grid, padding_mode=\"zeros\", align_corners=False)\n        x = x.permute(0, 2, 3, 1)\n        x = x.reshape(*shape)\n        return x\n\n\ndef preprocess( ob, aug=None):\n        ob = ob.copy()\n        for k, v in ob.items():\n            if len(v.shape) >= 4:\n                ob[k] = ob[k] / 255.0 - 0.5\n                if aug:\n                    ob[k] = aug(ob[k])\n            elif True:\n                shape = ob[k].shape\n                ob[k] = ob[k].view(-1, shape[-1])\n                ob[k] = torch.cat([ob[k][:, :2] / 40 - 0.5, ob[k][:, 2:] / 10], -1)\n                ob[k] = ob[k].view(shape)\n        return ob\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flip(x, l=None):\n    \"\"\"Flip dimensions, BxT -> TxB.\"\"\"\n    if isinstance(x, dict):\n        return [{k: v[:, t] for k, v in x.items()} for t in range(l)]\n    else:\n        return x.transpose(0, 1)\n    \ndef soft_copy_network(target, source, tau):\n    \"\"\"Blends `target` and `source`: `target` = `tau` * `target` + `(1-tau)` * `source`.\"\"\"\n    for target_param, source_param in zip(target.parameters(), source.parameters()):\n        target_param.data.lerp_(source_param.data, 1 - tau)\n\n\ndef copy_network(target, source):\n    \"\"\"Copies network parameters: `target` = `source`.\"\"\"\n    soft_copy_network(target, source, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Copied Code Cell, for TanH Dist\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributions\nimport numpy as np\n\n\n# Identity\nclass Identity(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def mode(self):\n        return self.mean\n\n    def sample(self):\n        return self.mean.detach()\n\n    def rsample(self):\n        return self.mean\n\n\nclass TanhIdentity(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def mode(self):\n        return torch.tanh(self.mean)\n\n    def sample(self):\n        return torch.tanh(self.mean).detach()\n\n    def rsample(self):\n        return torch.tanh(self.mean)\n\n\n# Categorical\nclass Categorical(torch.distributions.Categorical):\n    def sample(self):\n        return super().sample().unsqueeze(-1)\n\n    def log_prob(self, actions):\n        return super().log_prob(actions.squeeze(-1)).unsqueeze(-1)\n\n    def entropy(self):\n        return super().entropy() * 10.0  # scailing\n\n    def mode(self):\n        return self.probs.argmax(dim=-1, keepdim=True)\n\n\n# Normal\nclass Normal(torch.distributions.Independent):\n    def __init__(self, mean, std, event_dim=0):\n        super().__init__(torch.distributions.Normal(mean, std), event_dim)\n\n    def mode(self):\n        return self.mean\n\n\nclass AddBias(nn.Module):\n    def __init__(self, bias):\n        super().__init__()\n        self._bias = nn.Parameter(bias.unsqueeze(1))\n\n    def forward(self, x):\n        if x.dim() == 2:\n            bias = self._bias.t().view(1, -1)\n        else:\n            bias = self._bias.t().view(1, -1, 1, 1)\n        return x + bias\n\n\nclass DiagGaussian(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self._device = torch.device(cfg.device)\n        self.logstd = AddBias(torch.zeros(cfg.action_size, device=self._device))\n\n    def forward(self, x):\n        logstd = self.logstd(torch.zeros_like(x))\n        return Normal(x, logstd.exp(), 1)\n\n\n# Tanh Normal\nclass TanhTransform(torch.distributions.TanhTransform):\n    def _inverse(self, y):\n        \"\"\"Clamp y for numerical stability.\"\"\"\n        dtype = y.dtype\n        y = torch.clamp(y.float(), -0.99999997, 0.99999997)\n        x = torch.atanh(y)\n        return x.type(dtype)\n\n\nclass TanhNormal_(torch.distributions.transformed_distribution.TransformedDistribution):\n    \"\"\"X ~ Normal(loc, scale).\n    Y ~ TanhNormal(loc, scale) = tanh(X).\n    \"\"\"\n\n    def __init__(self, loc, scale):\n        base_dist = torch.distributions.Normal(loc, scale)\n        super().__init__(base_dist, TanhTransform())\n\n    @property\n    def mean(self):\n        return self.base_dist.mean.tanh()\n\n\nclass TanhNormal(torch.distributions.Independent):\n    def __init__(self, mean, std, event_dim=0):\n        super().__init__(TanhNormal_(mean, std), event_dim)\n\n    def mode(self):\n        return self.mean\n\n    def entropy(self):\n        \"\"\"No analytic form. Instead, use entropy of Normal as proxy.\"\"\"\n        return self.base_dist.base_dist.entropy()\n\n\nclass SampleDist(nn.Module):\n    def __init__(self, dist, samples=100):\n        super().__init__()\n        self.base_dist = dist\n        self._samples = samples\n\n    def __getattr__(self, name):\n        return getattr(self.base_dist, name)\n\n    @property\n    def mean(self):\n        samples = self.base_dist.rsample((self._samples,))\n        return torch.mean(samples, 0)\n\n    def mode(self):\n        samples = self.base_dist.rsample((self._samples,))\n        log_prob = self.base_dist.log_prob(samples)\n        log_prob = log_prob.sum(tuple(range(1, len(log_prob.shape))))\n        return samples[torch.argmax(log_prob)]\n\n    def entropy(self):\n        samples = self.base_dist.rsample((self._samples,))\n        log_prob = self.base_dist.log_prob(samples)\n        return -torch.mean(log_prob, 0)\n\n    def kl(self, scale=1.0):\n        \"\"\"KL Divergence between `self.base_dist` and unit gaussian.\"\"\"\n        samples = self.base_dist.rsample((self._samples,))\n        log_prob = self.base_dist.log_prob(samples)\n        mean = torch.zeros_like(self.base_dist.mean)\n        std = torch.ones_like(self.base_dist.mean)\n        p = Normal(mean, std, len(self.base_dist.event_shape))\n        log_prob_p = p.log_prob(samples * scale)\n        return log_prob.mean(0) - log_prob_p.mean(0)\n\n\ndef mc_kl(p, q=None, n_samples=100, scale=1.0):\n    \"\"\"Computes monte-carlo estimate of KL divergence.\"\"\"\n    if q is None or q == \"tanh\":\n        mean = torch.zeros_like(p.mean)\n        std = torch.ones_like(p.mean)\n        if q is None:\n            q = Normal(mean, std, len(p.event_shape))\n        else:\n            q = TanhNormal(mean, std, len(p.event_shape))\n\n    samples = p.rsample((n_samples,))\n    log_prob_p = p.log_prob(samples)\n    log_prob_q = q.log_prob(samples * scale)\n    return log_prob_p.mean(0) - log_prob_q.mean(0)\n\n\ndef normal_kl(p, q=None):\n    \"\"\"Computes KL divergence based on base normal dist.\"\"\"\n    if q is None or q == \"tanh\":\n        mean = torch.zeros_like(p.mean)\n        std = torch.ones_like(p.mean)\n        if q is None:\n            q = Normal(mean, std, len(p.event_shape))\n        else:\n            q = TanhNormal(mean, std, len(p.event_shape))\n\n    p_base = torch.distributions.Independent(p.base_dist.base_dist, len(p.event_shape))\n    q_base = torch.distributions.Independent(q.base_dist.base_dist, len(q.event_shape))\n    return torch.distributions.kl.kl_divergence(p_base, q_base)\n\n\nclass MixedDistribution(nn.Module):\n    def __init__(self, base_dists):\n        super().__init__()\n        assert isinstance(base_dists, OrderedDict)\n        self.base_dists = base_dists\n\n    def __getitem__(self, key):\n        return self.base_dists[key]\n\n    def mode(self):\n        return OrderedDict([(k, dist.mode()) for k, dist in self.base_dists.items()])\n\n    def sample(self):\n        return OrderedDict([(k, dist.sample()) for k, dist in self.base_dists.items()])\n\n    def rsample(self):\n        return OrderedDict([(k, dist.rsample()) for k, dist in self.base_dists.items()])\n\n    def log_prob(self, x):\n        assert isinstance(x, dict)\n        log_prob = OrderedDict(\n            [(k, dist.log_prob(x[k])) for k, dist in self.base_dists.items()]\n        )\n        return torch.stack(list(log_prob.values()), -1).sum(-1, keepdim=True)\n\n    def entropy(self):\n        return sum([dist.entropy() for dist in self.base_dists.values()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#input dimension = horizon x action_dimension + (note, state_dim mi8 be diff than ob_dim, they mentioned state_dim to be 50 in cfg)state_dimension (action dim is 4, so if our horizon is 10 i.e 1 skill = 10 actions, then total action input len = 10x 2 = 20, and we add states to it so 10 x 2 + 50 = 70)\n#skill dimension is 10, defult given in the config file \n#to the forward function concatenate actions and states, for horizon step\n\n#In the og code they returned tanh normal distribution: return TanhNormal(mean, std, event_dim=1)\n#However, they calculate behavioural_cloning_loss on after sampling from dist and kl_div directly using the dist.\nclass SkillEncoder(nn.Module):\n    \n    def __init__(self, action_obs_dimension , skill_dimension , hidden_dimension ):\n        super(SkillEncoder,self).__init__()\n        self.input_dim = action_obs_dimension\n        self.skill_dim = skill_dimension\n        self.hidden_dims = hidden_dimension\n        \n        #Copied Code Begins\n        init_std = 5\n        self._raw_init_std = np.log(np.exp(init_std) - 1)\n        self._min_std = 1e-4\n        self._mean_scale = 5\n        #Copied Code Ends\n        \n        self.encoder = nn.Sequential(nn.Linear(self.input_dim,self.hidden_dims[0]),\n                                     nn.ELU(),\n                                     nn.Linear(self.hidden_dims[0], self.hidden_dims[1]),\n                                     nn.ELU(),\n                                     nn.Linear(self.hidden_dims[1], self.hidden_dims[2]),\n                                     nn.ELU(),\n                                     nn.Linear(self.hidden_dims[2], self.hidden_dims[3]),\n                                     nn.ELU())\n        \n        \n        self.mean = nn.Linear(self.hidden_dims[3], self.skill_dim)\n        self.std = nn.Linear(self.hidden_dims[3], self.skill_dim)\n        \n        self.activation = nn.ELU()\n        \n    def forward(self,trajectory):\n        \n        hidden_state = self.encoder(trajectory)\n        hidden_state = self.activation(hidden_state)\n        mean = self.mean(hidden_state)\n        std = self.std(hidden_state)\n        \n        #Copied Code Begins\n        std = F.softplus(std + self._raw_init_std) + self._min_std\n        mean = self._mean_scale * (mean / self._mean_scale).tanh()\n        #Copied Code Ends\n        \n        #return mean, variance\n        return TanhNormal(mean, std, event_dim=1)\n\n#paper has skill policy with 2 parameters (skill and state, not just skill), so code also has input dim to be skill_dim + state_dim (encoded_state_dim )\n#in code of forward as well, they concat ll_embed(encoded state) with action\nclass SkillPolicy(nn.Module):\n    \n    def __init__(self, conditioned_skill_dimension, action_dimension, hidden_dimension):\n        \n        super(SkillPolicy, self).__init__()\n        \n        #Copied Code Begins\n        init_std = 5\n        self._raw_init_std = np.log(np.exp(init_std) - 1)\n        self._min_std = 1e-4\n        self._mean_scale = 5\n        #Copied Code Ends\n        \n        self.input_dimension = conditioned_skill_dimension\n        self.output_dimension = action_dimension\n        self.hidden_dims = hidden_dimension\n        \"\"\"self.decoder = nn.Sequential(nn.Linear(self.input_dimension,self.hidden_dims[0]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[0], self.hidden_dims[1]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[1], self.hidden_dims[2]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[2], self.hidden_dims[3]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[3], self.output_dimension))\"\"\"\n        self.decoder = nn.Sequential(nn.Linear(self.input_dimension,self.hidden_dims[0]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[0], self.hidden_dims[1]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[1], self.hidden_dims[2]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[2], self.hidden_dims[3]),\n                                    nn.ELU())\n        self.mean = nn.Linear(self.hidden_dims[3], self.output_dimension)\n        self.std = nn.Linear(self.hidden_dims[3], self.output_dimension)\n        \n        \n    def forward(self,skill_latent, state_latent, deterministic = False, return_dist = False):\n        \n        conditioned_latent = torch.cat([skill_latent,state_latent], -1)\n        \n        \n        actions = self.decoder(conditioned_latent)\n        mean = self.mean(actions)\n        std = self.std(actions)\n        std = F.softplus(std + self._raw_init_std) + self._min_std\n        mean = self._mean_scale * (mean / self._mean_scale).tanh()\n        dist = TanhNormal(mean, std, event_dim=1)\n        \n        action = dist.mode() if deterministic else dist.rsample()\n        if return_dist:\n            return action, dist\n        return action\n        \n        #return actions\n\nclass StateEncoder(nn.Module):\n    \n    def __init__(self, state_dimension, encoded_dimension, hidden_dimension):\n        \n        super(StateEncoder, self).__init__()\n        self.input_dimension = state_dimension\n        self.encoded_dimension = encoded_dimension\n        self.hidden_dims = hidden_dimension\n        self.encoder = nn.Sequential(nn.Linear(self.input_dimension,self.hidden_dims[0]),\n                                    nn.ReLU(),\n                                    nn.Linear(self.hidden_dims[0], self.hidden_dims[1]),\n                                    nn.ReLU(),\n                                    nn.Linear(self.hidden_dims[1], self.hidden_dims[2]),\n                                    nn.ReLU(),\n                                    nn.Linear(self.hidden_dims[2], self.hidden_dims[3]),\n                                    nn.ReLU(),\n                                    nn.Linear(self.hidden_dims[3], self.encoded_dimension))\n        self.activation = nn.ReLU()\n        \n    def forward(self,state):\n        \n        hidden = self.encoder(state)\n        out = self.activation(hidden)\n        \n        \n        \n        return out\n\nclass ObservationDecoder(nn.Module):\n    \n    def __init__(self, encoded_dimension, observation_dimension, hidden_dimension):\n        super(ObservationDecoder, self).__init__()\n        self.output_dimension = observation_dimension\n        self.input_dimension = encoded_dimension\n        self.hidden_dims = hidden_dimension\n        self.decoder = nn.Sequential(nn.Linear(self.input_dimension,self.hidden_dims[0]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[0], self.hidden_dims[1]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[1], self.hidden_dims[2]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[2], self.hidden_dims[3]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[3], self.output_dimension))\n        \n    def forward(self, state_latent):\n        \n        observation = self.decoder(state_latent)\n        return observation\n    \nclass SkillPrior(nn.Module):\n    \n    def __init__(self, initial_state_dim, encoded_dimension, hidden_dimension):\n        \n        super(SkillPrior, self).__init__()\n        \n        init_std = 5\n        self._raw_init_std = np.log(np.exp(init_std) - 1)\n        self._min_std = 1e-4\n        self._mean_scale = 5\n        \n        self.input_dim = initial_state_dim\n        self.encoded_dimension = encoded_dimension\n        self.hidden_dims = hidden_dimension\n        self.encoder = nn.Sequential(nn.Linear(self.input_dim,self.hidden_dims[0]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[0], self.hidden_dims[1]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[1], self.hidden_dims[2]),\n                                    nn.ELU(),\n                                    nn.Linear(self.hidden_dims[2], self.hidden_dims[3]),\n                                    nn.ELU())\n        \n        self.mean = nn.Linear(self.hidden_dims[3], self.encoded_dimension)\n        self.std = nn.Linear(self.hidden_dims[3], self.encoded_dimension)\n        self.activation = nn.ReLU()\n        \n    def forward(self,state):\n        \n        hidden = self.encoder(state)\n        hidden = self.activation(hidden)\n        mean = self.mean(hidden)\n        std = self.std(hidden)\n        std = F.softplus(std + self._raw_init_std) + self._min_std\n        mean = self._mean_scale * (mean / self._mean_scale).tanh()\n        return TanhNormal(mean, std, event_dim=1)\n        \n        #return mean, var\n    \nclass SkillDynamics(nn.Module):\n    \n    def __init__(self, skill_and_embed_dim, embed_state_dim, hidden_dim):\n        \n        super(SkillDynamics, self).__init__()\n        self.input_dim = skill_and_embed_dim\n        self.output_dim = embed_state_dim\n        self.hidden_dims = hidden_dim\n        self.dynamics  = nn.Sequential(nn.Linear(self.input_dim,self.hidden_dims[0]),\n                                      nn.ReLU(),\n                                      nn.Linear(self.hidden_dims[0], self.hidden_dims[1]),\n                                      nn.ReLU(),\n                                      nn.Linear(self.hidden_dims[1], self.hidden_dims[2]),\n                                      nn.ReLU(),\n                                      nn.Linear(self.hidden_dims[2], self.hidden_dims[3]),\n                                      nn.ReLU(),\n                                      nn.Linear(self.hidden_dims[3], self.output_dim),\n                                      nn.ReLU())\n        \n    def forward(self, embed_state, skill):\n        \n        inp = torch.cat([embed_state, skill], dim = -1)\n        next_embed_state = self.dynamics(inp)\n        \n        return next_embed_state\n    \nclass OfflineLearningModel(nn.Module):\n    \n    def __init__(self, trajectory_dim, skill_dim, action_dim, state_dim, embed_state_dim, hidden_dims):\n        \n        super(OfflineLearningModel, self).__init__()\n        self.skill_encoder = SkillEncoder(trajectory_dim, skill_dim , hidden_dims)\n        self.skill_policy = SkillPolicy(skill_dim, action_dim, hidden_dims)\n        self.state_encoder = StateEncoder(state_dim, embed_state_dim, hidden_dims)\n        self.skill_prior = SkillPrior(state_dim, embed_state_dim, hidden_dims)\n        self.observation_decoder = ObservationDecoder(embed_state_dim, state_dim, hidden_dims)\n        self.dynamics_model = SkillDynamics(skill_dim+embed_state_dim, embed_state_dim, hidden_dims)\n        \n        \n    def reparameterization(self, mean, variance):\n        \n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mean + eps * variance\n        \n    def forward(self,trajectories, initial_state, states, actions):\n        \n        skill = self.reparameterization(self.skill_encoder(trajectories))\n        reconstructed_actions = self.skill_policy(skill)\n        encoded_state = self.state_encoder(states[0])\n        reconstructed_state = self.observation_decoder(encoded_state)\n        next_embed_state = self.dynamics_model(encoded_state, skill)\n        skill_prior_embed_state = self.skill_prior(initial_state)\n        true_final_embed_state = self.reparameterization(self.state_encoder(states[-1]))\n        \n        return skill, reconstructed_actions, encoded_state, reconstructed_state, next_embed_state, skill_prior_embed_state, true_final_embed_state\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse = nn.MSELoss(reduction=\"none\")\n\nstate_dimension = 4\nencoded_dimension = 50\nhidden_dimension = [256,256,256, 256]\nstate_encoder = StateEncoder(state_dimension, encoded_dimension, hidden_dimension)\n\n#  state =state_encoder(ob)\n\n\naction_obs_dimension = skill_input_dim #70\nskill_dimension = skill_dim #10\nhidden_dimension = [256,256,256,256]\nskill_encoder = SkillEncoder(action_obs_dimension , skill_dimension , hidden_dimension)\n\nconditioned_skill_dim = state_dim + skill_dim #50+10\nskill_policy = SkillPolicy(conditioned_skill_dim, ac_dim, hidden_dimension)\n\nstate_enc = StateEncoder(state_dimension, encoded_dimension, hidden_dimension)\ntarget_state_enc = StateEncoder(state_dimension, encoded_dimension, hidden_dimension)\ncopy_network(target_state_enc, state_enc)\n\nobs_decoder = ObservationDecoder(encoded_dimension, state_dimension, hidden_dimension)\ndynamics = SkillDynamics(encoded_dimension+10, encoded_dimension, hidden_dimension)\nskill_prior = SkillPrior(encoded_dimension,skill_dim, hidden_dimension)\n\nhl_losses = []\nll_losses = []\nhl_actor_losses = []\nhl_model_losses = []\nll_actor_losses = []\nll_vae_kl_losses = []\nvae_kl_divs = []\nconsistency_losses = []\nhl_recon_losses = []\nactor_stds = []\nlosses = []\n\nB, H, L = batch_size, skill_horizon, n_skill\nbatch_size =128\ndef flip(x, l=None):\n            \"\"\"Flip dimensions, BxT -> TxB.\"\"\"\n            if isinstance(x, dict):\n                return [{k: v[:, t] for k, v in x.items()} for t in range(l)]\n            else:\n                return x.transpose(0, 1)\n\n\n\"\"\"\ndef train(self, batch, is_train=True):\n    \n\n    # ob: Bx(LxH+1)x`ob_dim`, ac: Bx(LxH+1)x`ac_dim`\n    ob, ac = batch[\"ob\"], batch[\"ac\"]\n    o = dict(ob=ob)\n    o = preprocess(o, aug=self._aug)\n    if ac.shape[1] == L * H + 1:\n        ac = ac[:, :-1, :]\n\n    with torch.autocast(device, enabled=True):\n        # Trains skill policy and skill embedding space.\n        \n        encoded_state = state_encoder(o['ob'])\n        \n        x = ac.view(B, L, -1)\n        x = torch.cat([x, encoded_state[:, :-1:H]], dim=-1)\n        \n        \n        skill_dist = skill_encoder(x)\n        sampled_skill = skill_dist.rsample()\n\n        if True:\n            # LL behavioral cloning loss.\n            z_repeat = sampled_skill.unsqueeze(-2).expand(-1, -1, H, -1)\n            \n            \n            ac_pred = skill_policy(encoded_state[:, :-1, :].view(B, L, H, -1), z_repeat, deterministic=True)\n            \n            \n            ll_actor_loss = (\n                1 * mse(ac_pred, ac.view(ac_pred.shape)).mean()\n            )\n\n            # LL embedding regularization loss.\n            vae_kl_div = mc_kl(skill_dist, \"tanh\")\n            vae_kl_div_clipped = torch.clamp(\n                vae_kl_div, -100, 100\n            ).mean()\n            \n            ll_vae_kl_loss = 1e-4 * vae_kl_div_clipped.mean()\n        \n\n        \n\n        # Trains skill dynamics model and skill prior.\n        hl_o = dict(ob=o[\"ob\"][:, ::H])\n        \n        \n        \n        \n\n        hl_feat = flip(state_encoder(hl_o['ob']))\n        with torch.no_grad():\n            hl_feat_target = flip(target_state_enc(hl_o['ob']))\n        hl_ac = flip(sampled_skill)\n        \n        \n\n        # HL observation reconstruction loss.\n        \n        \n        obs_pred = obs_decoder(hl_feat)\n        hl_recon_losses = mse(flip(hl_o),obs_pred).sum()\n        \n        h = h_next_pred = hl_feat[0]\n        consistency_loss = 0\n        hs = [h]\n        hl_o = flip(hl_o, L + 1)\n        \n        \n        joint_training = True\n\n        \n        for t in range(L):\n            h = h_next_pred\n            a = hl_ac[t] if joint_training else hl_ac[t].detach()\n            h_next_pred = dynamics(h, a)\n            h_next_target = hl_feat_target[t + 1]\n            rho = 0.5 ** t\n            consistency_loss += rho * mse(h_next_pred, h_next_target).mean(dim=1)\n            hs.append(h_next_pred)\n        \n        \n        hl_model_loss = (\n            1 * hl_recon_loss\n            + 2 * consistency_loss.clamp(max=1e4).mean()\n        )\n        hl_model_loss.register_hook(lambda grad: grad * (1 / L))\n        \n    \n    \n\n    # HL skill prior loss.\n    if not joint_training:\n        skill_prior_dist = skill_prior(hl_feat[:-1].detach())\n    else:\n        skill_prior_dist = skill_prior(hl_feat[:-1])\n    skill_dist_detached = TanhNormal(\n        flip(skill_dist.base_dist.base_dist.loc.detach()),\n        flip(skill_dist.base_dist.base_dist.scale.detach()),\n        1,\n    )\n    \n    \n    hl_actor_loss = 1 * mc_kl(skill_dist_detached, skill_prior_dist).mean()\n\n    hl_loss = hl_actor_loss + hl_model_loss\n    ll_loss = ll_actor_loss + ll_vae_kl_loss\n    total_loss = hl_loss + ll_loss\n    \n    hl_losses.append(hl_loss)\n    ll_losses.append(ll_loss)\n    hl_actor_losses.append(hl_actor_loss)\n    hl_model_losses.append(hl_model_loss)\n    ll_actor_losses.append(ll_actor_loss)\n    ll_vae_kl_losses.append(ll_vae_kl_loss)\n    vae_kl_divs.append(vae_kl_div_clipped.mean())\n    consistency_losses.append(consistency_loss.mean())\n    hl_recon_losses.append(hl_recon_loss)\n    actor_stds.append(skill_prior_dist.base_dist.base_dist.scale.mean())\n    losses.append(total_loss)\n    \n    \n    \n    \n    \n    \n    \n    info[\"hl_loss\"] = hl_loss.item()\n    info[\"ll_loss\"] = ll_loss.item()\n    info[\"hl_actor_loss\"] = hl_actor_loss.item()\n    info[\"hl_model_loss\"] = hl_model_loss.item()\n    info[\"ll_actor_loss\"] = ll_actor_loss.item()\n    info[\"ll_vae_kl_loss\"] = ll_vae_kl_loss.item()\n    info[\"vae_kl_div\"] = vae_kl_div_clipped.mean().item()\n    info[\"consistency_loss\"] = consistency_loss.mean().item()\n    info[\"hl_recon_loss\"] = hl_recon_loss.item()\n    info[\"actor_std\"] = skill_prior_dist.base_dist.base_dist.scale.mean().item()\n\n    if joint_training:\n        joint_grad_norm = self.joint_optim.step(total_loss)\n        info[\"joint_grad_norm\"] = joint_grad_norm.item()\n    else:\n        hl_model_grad_norm = self.hl_model_optim.step(hl_model_loss)\n        hl_actor_grad_norm = self.hl_actor_optim.step(hl_actor_loss)\n        ll_actor_grad_norm = self.ll_actor_optim.step(ll_loss)\n        info[\"hl_model_grad_norm\"] = hl_model_grad_norm.item()\n        info[\"hl_actor_grad_norm\"] = hl_actor_grad_norm.item()\n        info[\"ll_actor_grad_norm\"] = ll_actor_grad_norm.item()\n\n    if is_train:\n        self._update_iter += 1\n        # Update target networks.\n        if self._update_iter % cfg.target_update_freq == 0:\n            soft_copy_network(\n                hl_agent.model_target, hl_agent.model, cfg.target_update_tau\n            )\n    elif cfg.env == \"maze\":\n        # Visualize skill trajectories.\n        n_vis = cfg.pretrain.n_vis\n        info[\"model_prediction\"] = self._visualize(\n            o[\"ob\"][:n_vis].detach().cpu().numpy(),\n            hl_ob_pred.mode()[\"ob\"].transpose(0, 1)[:n_vis].detach().cpu().numpy(),\n        )\n\n        with torch.no_grad():\n            # HL rollout with learned skill dynamics.\n            hl_imagine_embed = torch.stack(hs, dim=0)[:, :n_vis].transpose(0, 1)\n            hl_imagine = hl_agent.decoder(hl_imagine_embed).mode()[\"ob\"]\n\n        info[\"skill_rollout\"] = self._visualize(\n            o[\"ob\"][:n_vis].detach().cpu().numpy(),\n            hl_imagine.detach().cpu().numpy(),\n        )\n\n    return info.get_dict()\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nepochs = 1000000\nbatch_size = 256\nall_parameters = list(state_encoder.parameters()) + list(skill_encoder.parameters()) + list(skill_policy.parameters()) + list(obs_decoder.parameters()) + list(dynamics.parameters()) + list(skill_prior.parameters())\nlearning_rate = 1e-3\nweight_decay = 0\naug = RandomShiftsAug()\noptimizer = torch.optim.AdamW(all_parameters, lr=learning_rate, weight_decay=weight_decay)\n\n\nmodels = [state_encoder, skill_encoder, skill_policy,obs_decoder, dynamics, skill_prior]\nfor epoch in range(epochs):\n    batch = pretrain_buffer.sample(batch_size)\n    \n    ob, ac = batch[\"ob\"], batch[\"ac\"]\n    o = dict(ob=ob)\n    o = preprocess(o, aug=aug)\n    if ac.shape[1] == L * H + 1:\n        ac = ac[:, :-1, :]\n\n    with torch.autocast(device, enabled=False):\n        # Trains skill policy and skill embedding space.\n        \n        encoded_state = state_encoder(o['ob'])\n        \n        x = ac.view(B, L, -1)\n        x = torch.cat([x, encoded_state[:, :-1:H]], dim=-1)\n        \n        \n        skill_dist = skill_encoder(x)\n        sampled_skill = skill_dist.rsample()\n\n        if True:\n            # LL behavioral cloning loss.\n            z_repeat = sampled_skill.unsqueeze(-2).expand(-1, -1, H, -1)\n            \n            \n            ac_pred = skill_policy(encoded_state[:, :-1, :].view(B, L, H, -1), z_repeat, deterministic=True)\n            \n            \n            ll_actor_loss = (\n                1 * mse(ac_pred, ac.view(ac_pred.shape)).mean()\n            )\n\n            # LL embedding regularization loss.\n            vae_kl_div = mc_kl(skill_dist, \"tanh\")\n            vae_kl_div_clipped = torch.clamp(\n                vae_kl_div, -100, 100\n            ).mean()\n            \n            ll_vae_kl_loss = 1e-4 * vae_kl_div_clipped.mean()\n        \n\n        \n\n        # Trains skill dynamics model and skill prior.\n        hl_o = dict(ob=o[\"ob\"][:, ::H])\n        \n        \n        \n        \n\n        hl_feat = flip(state_encoder(hl_o['ob']))\n        with torch.no_grad():\n            hl_feat_target = flip(target_state_enc(hl_o['ob']))\n        hl_ac = flip(sampled_skill)\n        \n        \n\n        # HL observation reconstruction loss.\n        \n        \n        obs_pred = obs_decoder(hl_feat)\n        hl_recon_loss = mse(flip(hl_o['ob']),obs_pred).sum()\n        \n        h = h_next_pred = hl_feat[0]\n        consistency_loss = 0\n        hs = [h]\n        hl_o = flip(hl_o, L + 1)\n        \n        \n        joint_training = True\n\n        \n        for t in range(L):\n            h = h_next_pred\n            a = hl_ac[t] if joint_training else hl_ac[t].detach()\n            h_next_pred = dynamics(h, a)\n            h_next_target = hl_feat_target[t + 1]\n            rho = 0.5 ** t\n            consistency_loss += rho * mse(h_next_pred, h_next_target).mean(dim=1)\n            hs.append(h_next_pred)\n        \n        \n        hl_model_loss = (\n            1 * hl_recon_loss\n            + 2 * consistency_loss.clamp(max=1e4).mean()\n        )\n        hl_model_loss.register_hook(lambda grad: grad * (1 / L))\n        \n    \n    \n\n    # HL skill prior loss.\n    if not joint_training:\n        skill_prior_dist = skill_prior(hl_feat[:-1].detach())\n    else:\n        skill_prior_dist = skill_prior(hl_feat[:-1])\n    skill_dist_detached = TanhNormal(\n        flip(skill_dist.base_dist.base_dist.loc.detach()),\n        flip(skill_dist.base_dist.base_dist.scale.detach()),\n        1,\n    )\n    \n    \n    hl_actor_loss = 1 * mc_kl(skill_dist_detached, skill_prior_dist).mean()\n\n    hl_loss = hl_actor_loss + hl_model_loss\n    ll_loss = ll_actor_loss + ll_vae_kl_loss\n    total_loss = hl_loss + ll_loss\n    \n    hl_losses.append(hl_loss)\n    ll_losses.append(ll_loss)\n    hl_actor_losses.append(hl_actor_loss)\n    hl_model_losses.append(hl_model_loss)\n    ll_actor_losses.append(ll_actor_loss)\n    ll_vae_kl_losses.append(ll_vae_kl_loss)\n    vae_kl_divs.append(vae_kl_div_clipped.mean())\n    consistency_losses.append(consistency_loss.mean())\n    hl_recon_losses.append(hl_recon_loss)\n    actor_stds.append(skill_prior_dist.base_dist.base_dist.scale.mean())\n    losses.append(total_loss)\n    \n    \n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    print(f'Epoch: {epoch+1},  Loss:{total_loss}')\n    \n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}